{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data variables:\n",
      "    trajectory      |S1 ...\n",
      "    launch_time     int32 ...\n",
      "    pres            (time) float32 ...\n",
      "    tdry            (time) float32 ...\n",
      "    dp              (time) float32 ...\n",
      "    rh              (time) float32 ...\n",
      "    u_wind          (time) float32 ...\n",
      "    v_wind          (time) float32 ...\n",
      "    w_wind          (time) float32 ...\n",
      "    wspd            (time) float32 ...\n",
      "    wdir            (time) float32 ...\n",
      "    dz              (time) float32 ...\n",
      "    mr              (time) float32 ...\n",
      "    vt              (time) float32 ...\n",
      "    theta           (time) float32 ...\n",
      "    theta_e         (time) float32 ...\n",
      "    theta_v         (time) float32 ...\n",
      "    lat             (time) float32 ...\n",
      "    lon             (time) float32 ...\n",
      "    alt             (time) float32 ...\n",
      "    gpsalt          (time) float32 ...\n",
      "    reference_time  (obs) int32 ...\n",
      "    reference_pres  (obs) float32 ...\n",
      "    reference_tdry  (obs) float32 ...\n",
      "    reference_rh    (obs) float32 ...\n",
      "    reference_wspd  (obs) float32 ...\n",
      "    reference_wdir  (obs) float32 ...\n",
      "    reference_lat   (obs) float32 ...\n",
      "    reference_lon   (obs) float32 ...\n",
      "    reference_alt   (obs) float32 ...\n",
      "----------\n",
      "Conventions\n",
      "RepoRevision\n",
      "RepoLastChangedDate\n",
      "RepoId\n",
      "RepoBranch\n",
      "featureType\n",
      "Agency\n",
      "AltInterpSpan\n",
      "AspenVersion\n",
      "BlendLength\n",
      "CabinTemp\n",
      "ChuteArea\n",
      "ConfigSetName\n",
      "DatasetDOI\n",
      "DiscardBadCrcData\n",
      "DoQC\n",
      "DragCoef\n",
      "DropSondeMass\n",
      "DropsondeHitSfc\n",
      "Flight\n",
      "GPSAltBuddySlope\n",
      "GPSAltQCDev\n",
      "GPSAltQCWL\n",
      "GPSPosBuddySlope\n",
      "GPSPosQCDev\n",
      "GPSPosQCWL\n",
      "IgnoreLaunchAlt\n",
      "IgnoreLaunchLat\n",
      "IgnoreLaunchLon\n",
      "IgnoreLaunchPres\n",
      "IgnoreLaunchRH\n",
      "IgnoreLaunchTdry\n",
      "IgnoreLaunchWdir\n",
      "IgnoreLaunchWspd\n",
      "ObsNum\n",
      "PlatformId\n",
      "PlatformType\n",
      "PosInterpSpan\n",
      "PresBuddySlope\n",
      "PresEquilTime\n",
      "PresEquilTimeOverride\n",
      "PresMonoCheck\n",
      "PresOffset\n",
      "PresOutlier\n",
      "PresQCDev\n",
      "PresQCWL\n",
      "PresSmoothWL\n",
      "ProcessingTime\n",
      "Project\n",
      "QCDisclaimer\n",
      "RHBuddySlope\n",
      "RHEquilTime\n",
      "RHEquilTimeOverride\n",
      "RHOffset\n",
      "RHOutlier\n",
      "RHQCDev\n",
      "RHQCWL\n",
      "RHSmoothWL\n",
      "RHThreshold\n",
      "RHchoice\n",
      "ReportObservedPos\n",
      "SetHtsMissing\n",
      "SfcAltUnknown\n",
      "SfcAltitude\n",
      "SondeId\n",
      "SoundingDescription\n",
      "TdryBuddySlope\n",
      "TdryDynCor\n",
      "TdryDynCorWL\n",
      "TdryEquilTime\n",
      "TdryEquilTimeOverride\n",
      "TdryOffset\n",
      "TdryOutlier\n",
      "TdryQCDev\n",
      "TdryQCWL\n",
      "TdrySmoothWL\n",
      "UseIgnoredLaunch\n",
      "UseTheoryVentRate\n",
      "WindBuddySlope\n",
      "WindDisableW\n",
      "WindDynCor\n",
      "WindDynCorWL\n",
      "WindEquilTime\n",
      "WindErrorHigh\n",
      "WindErrorLow\n",
      "WindOutlier\n",
      "WindQCDev\n",
      "WindQCWL\n",
      "WindSats\n",
      "WindSmoothWL\n",
      "WindVVPresWL\n",
      "WindVVdelta\n"
     ]
    }
   ],
   "source": [
    "with xr.open_dataset(\"../../test_data/CPEXAW-DROPSONDE_D20210806_193025_PQC.nc\", decode_cf=False) as ds:\n",
    "        print(ds.data_vars)\n",
    "        print(\"----------\")\n",
    "        for key in ds.attrs:\n",
    "                print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2979 - (2979,)\n",
      "2979 - (2979,)\n",
      "2979 - (2979,)\n",
      "2979 - (2979,)\n",
      "2979 - (2979,)\n",
      "2979 - (2979,)\n",
      "2979 - (2979,)\n",
      "-999.0\n",
      "-999.0\n",
      "-999.0\n"
     ]
    }
   ],
   "source": [
    "with xr.open_dataset(\"../../test_data/CPEXAW-DROPSONDE_D20210820_203208_PQC.nc\", decode_cf=False) as ds:\n",
    "        rh = ds['rh'].values # relative humidity\n",
    "        dp = ds['dp'].values # dew point\n",
    "        tdry = ds['tdry'].values # temp dry???\n",
    "        lat = ds['lat'].values\n",
    "        lon = ds['lon'].values\n",
    "        alt = ds['alt'].values\n",
    "        time = ds['time'].values\n",
    "\n",
    "        print(len(rh), \"-\", (rh.shape)) #np ndarray\n",
    "        print(len(dp), \"-\", (dp.shape)) \n",
    "        print(len(tdry), \"-\", (tdry.shape)) \n",
    "        print(len(lat), \"-\", (lat.shape))\n",
    "        print(len(lon), \"-\", (lon.shape))\n",
    "        print(len(alt), \"-\", (alt.shape))   \n",
    "        print(len(time), \"-\", (time.shape)) \n",
    "\n",
    "        # print(time, len(time)) \n",
    "        print(lat[123])\n",
    "        print(lon[123])\n",
    "        print(alt[123])   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "netcdf to zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# META needed for ingest\n",
    "campaign = 'Olympex'\n",
    "collection = \"AirborneRadar\"\n",
    "dataset = \"gpmValidationOlympexcrs\"\n",
    "variables = [\"zku\"]\n",
    "renderers = [\"point_cloud\"]\n",
    "chunk = 262144\n",
    "to_rad = np.pi / 180\n",
    "to_deg = 180 / np.pi\n",
    "\n",
    "def ingest(folder, file, s3bucket):\n",
    "    \"\"\"\n",
    "    Converts Level 1B crs data from s3 to zarr file and then stores it in the provided folder\n",
    "    Args:\n",
    "        folder (string): name to hold the raw files.\n",
    "        file (string): the s3 url to the raw file.\n",
    "    \"\"\"\n",
    "    store = zarr.DirectoryStore(folder)\n",
    "    root = zarr.group(store=store)\n",
    "    \n",
    "    # Create empty rows for modified data    \n",
    "    z_chunk_id = root.create_dataset('chunk_id', shape=(0, 2), chunks=None, dtype=np.int64)\n",
    "    z_location = root.create_dataset('location', shape=(0, 3), chunks=(chunk, None), dtype=np.float32)\n",
    "    z_time = root.create_dataset('time', shape=(0), chunks=(chunk), dtype=np.int32)\n",
    "    z_vars = root.create_group('value')\n",
    "    z_ref = z_vars.create_dataset('ref', shape=(0), chunks=(chunk), dtype=np.float32)\n",
    "    n_time = np.array([], dtype=np.int64)\n",
    "\n",
    "    # date = file.split(\"_\")[2]\n",
    "    date = \"\"\n",
    "    base_time = \"\"\n",
    "    # base_time = np.datetime64('{}-{}-{}'.format(date[:4], date[4:6], date[6:]))\n",
    "\n",
    "    # open dataset.\n",
    "    with xr.open_dataset(\"../../test_data/CPEXAW-DROPSONDE_D20210820_203208_PQC.nc\", decode_cf=False) as ds:\n",
    "        rh = ds['rh'].values # relative humidity\n",
    "        dp = ds['dp'].values # dew point\n",
    "        tdry = ds['tdry'].values # temp dry???\n",
    "        lat = ds['lat'].values\n",
    "        lon = ds['lon'].values\n",
    "        alt = ds['alt'].values\n",
    "        time = ds['time'].values\n",
    "\n",
    "    # data formation\n",
    "    \n",
    "    ref = np.column_stack((rh, dp, tdry)).reshape(-1)    \n",
    "    # as 3 kind of data at a single point in 3d space(lon lat alt) in a given time\n",
    "    lon = np.repeat(lon, 3)\n",
    "    lat = np.repeat(lat, 3)\n",
    "    alt = np.repeat(alt, 3)\n",
    "    time = np.repeat(time, 3)\n",
    "    \n",
    "    print(ref.shape)\n",
    "    print(lon.shape)\n",
    "    print(lat.shape)\n",
    "    print(alt.shape)\n",
    "    print(time.shape)\n",
    "    \n",
    "    # sort data by time\n",
    "    sort_idx = np.argsort(time)\n",
    "\n",
    "    lon = lon[sort_idx]\n",
    "    lat = lat[sort_idx]\n",
    "    alt = alt[sort_idx]\n",
    "    ref = ref[sort_idx]\n",
    "    time = time[sort_idx]\n",
    "\n",
    "    # remove nan and infinite using mask ???\n",
    "    mask = np.logical_and(alt != -999.0, lon != -999.0, lat != -999.0)\n",
    "    # mask = np.logical_and(np.isfinite(ref), alt > 0, alt != -999.0, lon != -999.0, lat != -999.0)\n",
    "    lon = lon[mask]\n",
    "    lat = lat[mask]\n",
    "    alt = alt[mask]\n",
    "    ref = ref[mask]\n",
    "    time = time[mask]\n",
    "\n",
    "    # Now populate (append) the empty rows with modified data.\n",
    "    z_location.append(np.stack([lon, lat, alt], axis=-1))\n",
    "    z_ref.append(ref)\n",
    "    n_time = np.append(n_time, time)\n",
    "\n",
    "    idx = np.arange(0, n_time.size, chunk)\n",
    "    chunks = np.zeros(shape=(idx.size, 2), dtype=np.int64)\n",
    "    chunks[:, 0] = idx\n",
    "    chunks[:, 1] = n_time[idx]\n",
    "    z_chunk_id.append(chunks)\n",
    "\n",
    "    epoch = np.min(n_time)\n",
    "    n_time = (n_time - epoch).astype(np.int32)\n",
    "    z_time.append(n_time)\n",
    "\n",
    "    # save it.\n",
    "    root.attrs.put({\n",
    "        \"campaign\": campaign,\n",
    "        \"collection\": collection,\n",
    "        \"dataset\": dataset,\n",
    "        \"variables\": variables,\n",
    "        \"renderers\": renderers,\n",
    "        \"epoch\": int(epoch)\n",
    "\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8937,)\n",
      "(8937,)\n",
      "(8937,)\n",
      "(8937,)\n",
      "(8937,)\n"
     ]
    }
   ],
   "source": [
    "zarr_folder = \"./tmp/crs_olympex/zarr/dropsonde\"\n",
    "ingest(zarr_folder, \"\", \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate point cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from threading import Thread, Lock\n",
    "\n",
    "to_rad = np.pi / 180.0\n",
    "to_deg = 180.0 / np.pi\n",
    "\n",
    "steps = [32, 16, 8, 4, 2, 1]\n",
    "\n",
    "class PointCloud:\n",
    "    def __init__(self, key, lon, lat, alt, value, time, epoch):\n",
    "        self.key = key\n",
    "        self.lon = lon\n",
    "        self.lat = lat\n",
    "        self.alt = alt\n",
    "        self.time = time\n",
    "        self.value = value\n",
    "        self.epoch = epoch\n",
    "        self.tasks = []\n",
    "        self.threads = []\n",
    "        for i in range(10):\n",
    "            self.threads.append(Thread(target=self.worker_function))\n",
    "        self.tileset_lock = Lock()\n",
    "        self.tileset_json = {\n",
    "        \t\"asset\": {\n",
    "        \t\t\"version\": \"1.0\",\n",
    "        \t\t\"type\": \"Airborne Radar\"\n",
    "        \t},\n",
    "        \t\"root\": {\n",
    "        \t\t\"geometricError\": 1000000,\n",
    "        \t\t\"refine\" : \"REPLACE\",\n",
    "        \t\t\"boundingVolume\": {\n",
    "                    \"region\": [\n",
    "                        float(np.min(lon)) * to_rad,\n",
    "                        float(np.min(lat)) * to_rad,\n",
    "                        float(np.max(lon)) * to_rad,\n",
    "                        float(np.max(lat)) * to_rad,\n",
    "                        float(np.min(alt)) * to_rad,\n",
    "                        float(np.max(alt)) * to_rad\n",
    "                    ]\n",
    "                },\n",
    "                \"children\": []\n",
    "        \t},\n",
    "            \"properties\": {\n",
    "                \"epoch\": \"{}Z\".format(datetime.utcfromtimestamp(epoch).isoformat()),\n",
    "                \"refined\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def worker_function(self):\n",
    "        while len(self.tasks) > 0:\n",
    "                tile, start, end = self.tasks.pop()\n",
    "                print(tile, start, end)\n",
    "                self.generate(tile, start, end)\n",
    "\n",
    "\n",
    "    def start(self):\n",
    "        for t in self.threads:\n",
    "            t.start()\n",
    "\n",
    "\n",
    "    def join(self):\n",
    "        for t in self.threads:\n",
    "            t.join()\n",
    "        with open('{}/tileset.json'.format(self.key), mode='w+') as outfile:\n",
    "            json.dump(self.tileset_json, outfile)\n",
    "\n",
    "\n",
    "    def schedule_task(self, tile, start, end):\n",
    "        self.tasks.append((tile, start, end))\n",
    "\n",
    "\n",
    "    def generate(self, tile, start, end):\n",
    "        print(tile, start, end)\n",
    "        parent_tile = self.tileset_json[\"root\"]\n",
    "        cartesian, offset, scale, cartographic, region = self.cartographic_to_cartesian(start, end)\n",
    "\n",
    "        value = self.value[start:end]\n",
    "        time = self.time[start:end]\n",
    "\n",
    "        epoch = int(np.min(time) + self.epoch - 300)\n",
    "        epoch = \"{}Z\".format(datetime.utcfromtimestamp(epoch).isoformat())\n",
    "        end = int(np.max(time) + self.epoch + 300)\n",
    "        end = \"{}Z\".format(datetime.utcfromtimestamp(end).isoformat())\n",
    "\n",
    "        header_length = 28\n",
    "        magic = np.string_(\"pnts\")\n",
    "        version = 1\n",
    "\n",
    "        for step in steps:\n",
    "            self.tileset_lock.acquire()\n",
    "            try:\n",
    "                filename = \"{}_{}.pnts\".format(tile, step)\n",
    "                child_tile = {\n",
    "                    \"availability\": \"{}/{}\".format(epoch, end),\n",
    "                    \"geometricError\": step * 500,\n",
    "                    \"boundingVolume\": {\n",
    "                        \"region\": region\n",
    "                    },\n",
    "                    \"content\": {\n",
    "                        \"uri\": filename\n",
    "                    },\n",
    "                    \"refine\": \"REPLACE\"\n",
    "                }\n",
    "                if step == 1:\n",
    "                    self.tileset_json[\"properties\"][\"refined\"].append(filename)\n",
    "                else:\n",
    "                    child_tile[\"children\"] = []\n",
    "                parent_tile[\"children\"].append(child_tile)\n",
    "                parent_tile = child_tile\n",
    "            finally:\n",
    "                self.tileset_lock.release()\n",
    "\n",
    "            tile_length = 0\n",
    "            feature_table_binary_byte_length = 0\n",
    "            batch_table_binary_byte_length = 0\n",
    "            length = value[::step].size\n",
    "\n",
    "            feature_table_json = {\n",
    "                \"POINTS_LENGTH\": length,\n",
    "                \"BATCH_LENGTH\": length,\n",
    "                \"BATCH_ID\": {\n",
    "                    \"byteOffset\": 0,\n",
    "                    \"componentType\": \"UNSIGNED_INT\"\n",
    "                },\n",
    "                \"POSITION_QUANTIZED\": {\n",
    "                    \"byteOffset\": length * 4\n",
    "                },\n",
    "                \"QUANTIZED_VOLUME_OFFSET\": offset,\n",
    "                \"QUANTIZED_VOLUME_SCALE\": scale\n",
    "            }\n",
    "\n",
    "            batch_table_json = {\n",
    "                \"value\": {\n",
    "                    \"byteOffset\": 0,\n",
    "                    \"componentType\": \"FLOAT\",\n",
    "                    \"type\": \"SCALAR\"\n",
    "                },\n",
    "                \"time\": {\n",
    "                    \"byteOffset\": length * 4,\n",
    "                    \"componentType\": \"FLOAT\",\n",
    "                    \"type\": \"SCALAR\"\n",
    "                },\n",
    "                \"location\": {\n",
    "                    \"byteOffset\": length * 8,\n",
    "                    \"componentType\": \"SHORT\",\n",
    "                    \"type\": \"VEC3\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "            tile_length += header_length\n",
    "\n",
    "            feature_table_json_min = json.dumps(feature_table_json, separators=(\",\", \":\")) + \"       \"\n",
    "            feature_table_trim = (tile_length + len(feature_table_json_min)) % 8\n",
    "            if feature_table_trim != 0:\n",
    "                feature_table_json_min = feature_table_json_min[:-feature_table_trim]\n",
    "\n",
    "            tile_length += len(feature_table_json_min)\n",
    "\n",
    "            feature_table_binary_byte_length = length * 4 + length * 3 * 2\n",
    "            tile_length += feature_table_binary_byte_length\n",
    "            feature_table_padding = tile_length % 8\n",
    "            if feature_table_padding != 0:\n",
    "                feature_table_padding = 8 - feature_table_padding\n",
    "            tile_length += feature_table_padding\n",
    "\n",
    "            batch_table_json_min = json.dumps(batch_table_json, separators=(\",\", \":\")) + \"       \"\n",
    "            batch_table_trim = (tile_length + len(batch_table_json_min)) % 8\n",
    "            if batch_table_trim != 0:\n",
    "                batch_table_json_min = batch_table_json_min[:-batch_table_trim]\n",
    "\n",
    "            tile_length += len(batch_table_json_min)\n",
    "\n",
    "            batch_table_binary_byte_length = length * 4 * 2 + length * 2 * 3\n",
    "            tile_length += batch_table_binary_byte_length\n",
    "            batch_table_padding = tile_length % 8\n",
    "            if batch_table_padding != 0:\n",
    "                batch_table_padding = 8 - batch_table_padding\n",
    "            tile_length += batch_table_padding\n",
    "\n",
    "            with open('{}/{}'.format(self.key, filename), mode='wb+') as outfile:\n",
    "                outfile.write(np.string_(magic).tobytes())\n",
    "                outfile.write(np.uint32(version).tobytes())\n",
    "                outfile.write(np.uint32(tile_length).tobytes())\n",
    "                outfile.write(np.uint32(len(feature_table_json_min)).tobytes())\n",
    "                outfile.write(np.uint32(feature_table_binary_byte_length + feature_table_padding).tobytes())\n",
    "                outfile.write(np.uint32(len(batch_table_json_min)).tobytes())\n",
    "                outfile.write(np.uint32(batch_table_binary_byte_length + batch_table_padding).tobytes())\n",
    "                outfile.write(np.string_(feature_table_json_min).tobytes())\n",
    "                outfile.write(np.arange(length, dtype=np.uint32).tobytes())\n",
    "                outfile.write(cartesian[::step, :].tobytes())\n",
    "                for _ in range(feature_table_padding):\n",
    "                    outfile.write(np.string_(\" \").tobytes())\n",
    "                outfile.write(np.string_(batch_table_json_min).tobytes())\n",
    "                outfile.write(value[::step].astype(np.float32).tobytes())\n",
    "                outfile.write(time[::step].astype(np.float32).tobytes())\n",
    "                outfile.write(cartographic[::step, :].tobytes())\n",
    "                for _ in range(batch_table_padding):\n",
    "                    outfile.write(np.string_(\" \").tobytes())\n",
    "                outfile.seek(0)\n",
    "\n",
    "\n",
    "    def cartographic_to_cartesian(self, start, end):\n",
    "        lon = self.lon[start:end]\n",
    "        lat = self.lat[start:end]\n",
    "        alt = self.alt[start:end]\n",
    "        size = lon.size\n",
    "\n",
    "        cartographic = np.zeros(shape=(size, 3), dtype=np.int16)\n",
    "        cartographic[:, 0] = (lon * 32767 / 180).astype(np.int16)\n",
    "        cartographic[:, 1] = (lat * 32767 / 180).astype(np.int16)\n",
    "        cartographic[:, 2] = (alt / 10).astype(np.int16)\n",
    "\n",
    "        lon = lon * to_rad\n",
    "        lat = lat * to_rad\n",
    "\n",
    "        radiiSquared = np.array([40680631590769, 40680631590769, 40408299984661.445], dtype=np.float64)\n",
    "\n",
    "        N1 = np.multiply(np.cos(lat), np.cos(lon))\n",
    "        N2 = np.multiply(np.cos(lat), np.sin(lon))\n",
    "        N3 = np.sin(lat)\n",
    "\n",
    "        magnitude = np.sqrt(np.square(N1) + np.square(N2) + np.square(N3))\n",
    "\n",
    "        N1 = N1 / magnitude\n",
    "        N2 = N2 / magnitude\n",
    "        N3 = N3 / magnitude\n",
    "\n",
    "        K1 = radiiSquared[0] * N1\n",
    "        K2 = radiiSquared[1] * N2\n",
    "        K3 = radiiSquared[2] * N3\n",
    "\n",
    "        gamma = np.sqrt(np.multiply(N1, K1) + np.multiply(N2, K2) + np.multiply(N3, K3))\n",
    "\n",
    "        K1 = K1 / gamma\n",
    "        K2 = K2 / gamma\n",
    "        K3 = K3 / gamma\n",
    "\n",
    "        N1 = np.multiply(N1, alt)\n",
    "        N2 = np.multiply(N2, alt)\n",
    "        N3 = np.multiply(N3, alt)\n",
    "\n",
    "        # x = np.multiply((N1 + K1), np.random.normal(1, .00005, N1.size))\n",
    "        # y = np.multiply((N2 + K2), np.random.normal(1, .00005, N1.size))\n",
    "        # z = np.multiply((N3 + K3), np.random.normal(1, .00005, N1.size))\n",
    "\n",
    "        x = N1 + K1\n",
    "        y = N2 + K2\n",
    "        z = N3 + K3\n",
    "\n",
    "        offset = [float(np.min(x)), float(np.min(y)), float(np.min(z))]\n",
    "\n",
    "        x = x - offset[0]\n",
    "        y = y - offset[1]\n",
    "        z = z - offset[2]\n",
    "\n",
    "        scale = [float(abs(np.max(x))), float(abs(np.max(y))), float(abs(np.max(z)))]\n",
    "\n",
    "        cartesian = np.zeros(shape=(size, 3), dtype=np.uint16)\n",
    "        cartesian[:, 0] = (x / scale[0] * 65535.0).astype(np.uint16)\n",
    "        cartesian[:, 1] = (y / scale[1] * 65535.0).astype(np.uint16)\n",
    "        cartesian[:, 2] = (z / scale[2] * 65535.0).astype(np.uint16)\n",
    "\n",
    "        region = [\n",
    "            float(np.min(lon)),\n",
    "            float(np.min(lat)),\n",
    "            float(np.max(lon)),\n",
    "            float(np.max(lat)),\n",
    "            float(np.min(alt)),\n",
    "            float(np.max(alt))\n",
    "        ]\n",
    "\n",
    "        return cartesian, offset, scale, cartographic, region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import zarr\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "to_rad = np.pi / 180.0\n",
    "to_deg = 180.0 / np.pi\n",
    "\n",
    "def generate_point_cloud(variable, epoch, end, zarr_location, point_cloud_folder):\n",
    "    \"\"\"Generates json pointcloud from a given zarr file input\n",
    "\n",
    "    Args:\n",
    "        variable (_type_): _description_\n",
    "        epoch (_type_): _description_\n",
    "        end (_type_): _description_\n",
    "        zarr_location (string): source zarr file.\n",
    "        point_cloud_folder (string): destination folder for 3d tile json file.\n",
    "    \"\"\"\n",
    "\n",
    "    #out_key = f\"{os.getenv('CRS_OUTPUT_FLIGHT_PATH')}/{shortname}\"\n",
    "    #pc_out_key = f\"{output_path}/point_cloud\"\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        os.mkdir(out_key)\n",
    "    except:\n",
    "        pass\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        os.mkdir(point_cloud_folder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # LOAD THE DATA.\n",
    "    store = zarr.DirectoryStore(zarr_location)\n",
    "    root = zarr.group(store=store)\n",
    "\n",
    "    chunk_id = root[\"chunk_id\"][:]\n",
    "    num_chunks = chunk_id.shape[0]\n",
    "    id = np.argmax(chunk_id[:, 1] > epoch) - 1\n",
    "    start_id = chunk_id[0 if id < 0 else id, 0]\n",
    "    id = num_chunks - np.argmax(chunk_id[::-1, 1] < end)\n",
    "    end_id = chunk_id[id, 0] if id < num_chunks else root[\"time\"].size - 1\n",
    "\n",
    "    root_epoch = root.attrs[\"epoch\"]\n",
    "    location = root[\"location\"][start_id:end_id]\n",
    "    lon = location[:, 0]\n",
    "    lat = location[:, 1]\n",
    "    alt = location[:, 2]\n",
    "    value = root[\"value\"][variable][start_id:end_id]\n",
    "    time = root[\"time\"][start_id:end_id]\n",
    "\n",
    "    # filter data using mask\n",
    "    epoch = epoch - root_epoch # date-time\n",
    "    end = end - root_epoch\n",
    "    mask = np.logical_and(time >= epoch, time <= end)\n",
    "    lon = lon[mask]\n",
    "    lat = lat[mask]\n",
    "    alt = alt[mask]\n",
    "    value = value[mask]\n",
    "    time = time[mask]\n",
    "\n",
    "    # Generate Pointcloud Tileset\n",
    "    point_cloud = PointCloud(point_cloud_folder, lon, lat, alt, value, time, root_epoch)\n",
    "\n",
    "    for tile in range(int(np.ceil(time.size / 530000))):\n",
    "        start_id = tile * 530000\n",
    "        end_id = np.min([start_id + 530000, time.size])\n",
    "        point_cloud.schedule_task(tile, start_id, end_id)\n",
    "\n",
    "    point_cloud.start()\n",
    "    point_cloud.join()\n",
    "\n",
    "tileset_json = {\n",
    "\t\"asset\": {\n",
    "\t\t\"version\": \"1.0\",\n",
    "\t\t\"type\": \"Airborne Radar\"\n",
    "\t},\n",
    "\t\"root\": {\n",
    "\t\t\"geometricError\": 1000000,\n",
    "\t\t\"refine\" : \"REPLACE\",\n",
    "\t\t\"boundingVolume\": {\n",
    "            \"region\": []\n",
    "        },\n",
    "        \"children\": []\n",
    "\t},\n",
    "    \"properties\": {\n",
    "        \"epoch\": \"\",\n",
    "        \"refined\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main(sys.argv[1], sys.argv[2], int(sys.argv[3]), int(sys.argv[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2474\n",
      "0 0 2474\n"
     ]
    }
   ],
   "source": [
    "point_cloud_folder = f\"{zarr_folder}/point_cloud\"\n",
    "generate_point_cloud(\"ref\",  0,  1000000000000, zarr_folder, point_cloud_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[[[5.68044561 9.25596638 0.71036058]\n",
      "  [0.871293   0.20218397 8.32619846]]\n",
      "\n",
      " [[7.78156751 8.70012148 9.78618342]\n",
      "  [7.99158564 4.61479362 7.80529176]]]\n",
      "[[-99.83, -99.32], [-99.79, -99.23]]\n",
      "[[42.25, 42.21], [42.63, 42.59]]\n",
      "DatetimeIndex(['2014-09-06', '2014-09-07', '2014-09-08'], dtype='datetime64[ns]', freq='D')\n",
      "2014-09-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "temperature = 15 + 8 * np.random.randn(2, 2, 3)\n",
    "precipitation = 10 * np.random.rand(2, 2, 3)\n",
    "lon = [[-99.83, -99.32], [-99.79, -99.23]]\n",
    "lat = [[42.25, 42.21], [42.63, 42.59]]\n",
    "time = pd.date_range(\"2014-09-06\", periods=3)\n",
    "reference_time = pd.Timestamp(\"2014-09-05\")\n",
    "print(temperature.shape)\n",
    "print(precipitation)\n",
    "print(lon)\n",
    "print(lat)\n",
    "print(time)\n",
    "print(reference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[1 1 1 2 2 2 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "print(x)\n",
    "xx = np.repeat(x, 3)\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 7, 2, 5, 8, 3, 6, 9])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "c = np.array([7,8,9])\n",
    "res = np.column_stack((a,b,c)).reshape(-1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsc-fcx-n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
